{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Usage:\n",
    "python eval.py --checkpoint data/image/pusht/diffusion_policy_cnn/train_0/checkpoints/latest.ckpt -o data/pusht_eval_output\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "# use line-buffering for both stdout and stderr\n",
    "# sys.stdout = open(sys.stdout.fileno(), mode='w', buffering=1)\n",
    "# sys.stderr = open(sys.stderr.fileno(), mode='w', buffering=1)\n",
    "\n",
    "import os\n",
    "import pathlib\n",
    "import hydra\n",
    "import torch\n",
    "import dill\n",
    "import wandb\n",
    "import json\n",
    "from diffusion_policy.workspace.base_workspace import BaseWorkspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ Setup ================================\n",
      "Workspace: TrainDiffusionTransformerHybridWorkspace\n",
      "policy: DiffusionTransformerHybridImagePolicy\n",
      "environment: PushTImageRunner\n",
      "\n",
      "============= Initialized Observation Utils with Obs Spec =============\n",
      "\n",
      "using obs modality: low_dim with keys: ['agent_pos']\n",
      "using obs modality: rgb with keys: ['image']\n",
      "using obs modality: depth with keys: []\n",
      "using obs modality: scan with keys: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shuyuan-19/miniforge-pypy3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/shuyuan-19/miniforge-pypy3/envs/robodiff/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop: 0.0\n",
      "----- Policy Setup -----\n",
      "T: 10, To: 2\n",
      "Da: 2, Do: 66\n",
      "Data Type: torch.float32, obs_as_cond: True\n",
      "pred_action_steps_only: False\n",
      "n_action_steps: 8\n",
      "pygame 2.1.2 (SDL 2.0.16, Python 3.9.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "----- Environment Setup -----\n",
      "n_obs_steps: 2, n_action_steps: 8\n",
      "n_train: 6,     n_test: 50\n",
      "max_step: 300\n",
      "========================== Action Prediction ==========================\n",
      "----- Step #1 -----\n",
      "Observation: \n",
      "  agent_pos: torch.Size([56, 2, 2])\n",
      "  image: torch.Size([56, 2, 3, 96, 96])\n",
      "B: 56\n",
      "nobs: <class 'dict'>\n",
      "value: torch.Size([56, 2, 2]) <class 'torch.Tensor'>\n",
      "this_nobs: <class 'dict'>\n",
      "nobs_features: torch.Size([112, 66]) <class 'torch.Tensor'>\n",
      " \n",
      "final input: \n",
      "  cond: torch.Size([56, 2, 66]) <class 'torch.Tensor'>\n",
      "  cond_data: torch.Size([56, 10, 2]) <class 'torch.Tensor'>\n",
      "  cond_mask: torch.Size([56, 10, 2]) <class 'torch.Tensor'>\n",
      " \n",
      "tensor([99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83, 82,\n",
      "        81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66, 65, 64,\n",
      "        63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49, 48, 47, 46,\n",
      "        45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28,\n",
      "        27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,\n",
      "         9,  8,  7,  6,  5,  4,  3,  2,  1,  0])\n",
      "timestep: 99\n",
      "timesteps: tensor([99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99,\n",
      "        99, 99], device='cuda:0')\n",
      "time_emb: torch.Size([56, 1, 256])\n",
      "input_emb: torch.Size([56, 10, 256])\n",
      " \n",
      "----- Encoder -----\n",
      "conda_obs_emb: torch.Size([56, 2, 256])\n",
      "cond_embeddings: torch.Size([56, 3, 256])\n",
      "position_embeddings: torch.Size([1, 3, 256])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TransformerForDiffusion' object has no attribute 'p_drop_emb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m     out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_log.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(json_log, \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m), indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, sort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 57\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 43\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(checkpoint, output_dir, device)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# run eval\u001b[39;00m\n\u001b[1;32m     40\u001b[0m env_runner \u001b[38;5;241m=\u001b[39m hydra\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39minstantiate(\n\u001b[1;32m     41\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mtask\u001b[38;5;241m.\u001b[39menv_runner,\n\u001b[1;32m     42\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[0;32m---> 43\u001b[0m runner_log \u001b[38;5;241m=\u001b[39m \u001b[43menv_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# dump log to json\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/diffusion_policy_learning/diffusion_policy/env_runner/pusht_image_runner.py:222\u001b[0m, in \u001b[0;36mPushTImageRunner.run\u001b[0;34m(self, policy)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# run policy\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 222\u001b[0m     action_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# device_transfer\u001b[39;00m\n\u001b[1;32m    225\u001b[0m np_action_dict \u001b[38;5;241m=\u001b[39m dict_apply(action_dict,\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/projects/diffusion_policy_learning/diffusion_policy/policy/diffusion_transformer_hybrid_image_policy.py:284\u001b[0m, in \u001b[0;36mDiffusionTransformerHybridImagePolicy.predict_action\u001b[0;34m(self, obs_dict)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# run sampling\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m nsample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconditional_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m# unnormalize prediction\u001b[39;00m\n\u001b[1;32m    291\u001b[0m naction_pred \u001b[38;5;241m=\u001b[39m nsample[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:Da]\n",
      "File \u001b[0;32m~/projects/diffusion_policy_learning/diffusion_policy/policy/diffusion_transformer_hybrid_image_policy.py:208\u001b[0m, in \u001b[0;36mDiffusionTransformerHybridImagePolicy.conditional_sample\u001b[0;34m(self, condition_data, condition_mask, cond, generator, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m trajectory[condition_mask] \u001b[38;5;241m=\u001b[39m condition_data[condition_mask]\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# 2. predict model output\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# 3. compute previous image: x_t -> x_t-1\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/diffusion_policy_learning/diffusion_policy/model/diffusion/transformer_for_diffusion.py:330\u001b[0m, in \u001b[0;36mTransformerForDiffusion.forward\u001b[0;34m(self, sample, timestep, cond, **kwargs)\u001b[0m\n\u001b[1;32m    326\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcond_pos_emb[\n\u001b[1;32m    327\u001b[0m     :, :tc, :\n\u001b[1;32m    328\u001b[0m ]  \u001b[38;5;66;03m# each position maps to a (learnable) vector\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mposition_embeddings: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(position_embeddings\u001b[38;5;241m.\u001b[39mshape))\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_drop_emb\u001b[49m)\n\u001b[1;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(cond_embeddings \u001b[38;5;241m+\u001b[39m position_embeddings)\n\u001b[1;32m    332\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n",
      "File \u001b[0;32m~/miniforge-pypy3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TransformerForDiffusion' object has no attribute 'p_drop_emb'"
     ]
    }
   ],
   "source": [
    "checkpoint = 'models/pre-trained/epoch=0100-test_mean_score=0.748.ckpt'\n",
    "output_dir = 'data/pusht_eval_output'\n",
    "device = 'cuda:0'\n",
    "\n",
    "# eval\n",
    "def main(checkpoint, output_dir, device):\n",
    "    print(\"================================ Setup ================================\")\n",
    "\n",
    "    # load checkpoint\n",
    "    payload = torch.load(open(checkpoint, 'rb'), pickle_module=dill)\n",
    "    cfg = payload['cfg']\n",
    "    cls = hydra.utils.get_class(cfg._target_)\n",
    "    print(\"Workspace: {}\".format(cfg._target_.split('.')[-1]))\n",
    "    print(\"policy: {}\".format(cfg.policy._target_.split('.')[-1]))\n",
    "    print(\"environment: {}\".format(cfg.task.env_runner._target_.split('.')[-1]))\n",
    "\n",
    "    # initialize workspace\n",
    "    workspace = cls(cfg, output_dir=output_dir)\n",
    "    workspace: BaseWorkspace\n",
    "    workspace.load_payload(payload, exclude_keys=None, include_keys=None)\n",
    "\n",
    "    # get policy from workspace\n",
    "    policy = workspace.model\n",
    "    if cfg.training.use_ema:\n",
    "        policy = workspace.ema_model\n",
    "        \n",
    "    device = torch.device(device)\n",
    "    policy.to(device)\n",
    "    policy.eval()\n",
    "\n",
    "    # log cfg of policy\n",
    "    print(\"----- Policy Setup -----\")\n",
    "    print(\"T: {}, To: {}\".format(policy.horizon, policy.n_obs_steps))\n",
    "    print(\"Da: {}, Do: {}\".format(policy.action_dim, policy.obs_feature_dim))\n",
    "    print(\"Data Type: {}, obs_as_cond: {}\".format(policy.dtype, policy.obs_as_cond))\n",
    "    print(\"pred_action_steps_only: {}\".format(policy.pred_action_steps_only))\n",
    "    print(\"n_action_steps: {}\".format(policy.n_action_steps))\n",
    "        \n",
    "    # run eval\n",
    "    env_runner = hydra.utils.instantiate(\n",
    "        cfg.task.env_runner,\n",
    "        output_dir=output_dir)\n",
    "    runner_log = env_runner.run(policy)\n",
    "\n",
    "    return\n",
    "    \n",
    "    # dump log to json\n",
    "    json_log = dict()\n",
    "    for key, value in runner_log.items():\n",
    "        if isinstance(value, wandb.sdk.data_types.video.Video):\n",
    "            json_log[key] = value._path\n",
    "        else:\n",
    "            json_log[key] = value\n",
    "    out_path = os.path.join(output_dir, 'eval_log.json')\n",
    "    json.dump(json_log, open(out_path, 'w'), indent=2, sort_keys=True)\n",
    "\n",
    "main(checkpoint, output_dir, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
